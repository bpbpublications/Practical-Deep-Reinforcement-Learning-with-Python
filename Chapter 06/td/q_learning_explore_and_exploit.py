import random
import numpy as np
from ch6.gym_maze.maze_env import MazeEnv
from ch6.maze.core import maze_states
from ch6.maze.map import map1
from ch6.td.policy import epsilon_greedy_policy, greedy_policy
from ch6.td.td import q_learning

actions = ['U', 'R', 'D', 'L']
x_coord, y_coord, blocks, goal = map1()

seed = 1
random.seed(seed)
env = MazeEnv(x_coord, y_coord, blocks, goal)
env.seed(seed)

states = maze_states(x_coord, y_coord)
Q = np.array(np.zeros([len(states), len(actions)]))

# Training
epsilon = .1
gamma = 0.75
alpha = 0.9
episodes = 50

for e in range(episodes):
    state = env.reset()
    i = 0
    while True:
        i += 1

        action_idx = epsilon_greedy_policy(epsilon, Q, states.index(state))
        action = actions[action_idx]
        next_state, reward, done, debug = env.step(action)

        Q = q_learning(
            Q,
            states.index(state),
            states.index(next_state),
            action_idx,
            reward,
            gamma,
            alpha
        )

        state = next_state

        if done:
            break

# Evaluation
state = env.reset()
while True:
    env.render()
    state_idx = states.index(state)
    action_idx = greedy_policy(Q, state_idx)
    action = actions[action_idx]
    state, reward, done, debug = env.step(action)

    if done:
        env.render()
        break

env.close()
